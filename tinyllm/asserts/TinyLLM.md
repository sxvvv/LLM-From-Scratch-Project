## ğŸ“‹ ç›®å½•

- é¡¹ç›®æ¦‚è¿°

- é¡¹ç›®æ¶æ„

- é¡¹ç›®æ„å»ºè¯¦ç»†æ‹†è§£

- è®­ç»ƒç»“æœ

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

### é¡¹ç›®ç®€ä»‹

TinyLLMæ˜¯ä¸€ä¸ªä»é›¶å¼€å§‹å®ç°çš„å°å‹è¯­è¨€æ¨¡å‹ï¼ŒåŸºäºTransformeræ¶æ„ï¼Œç”¨äºå­¦ä¹ å’Œç†è§£å¤§è¯­è¨€æ¨¡å‹çš„å·¥ä½œåŸç†ã€‚

### æŠ€æœ¯æ ˆ

- **æ·±åº¦å­¦ä¹ æ¡†æ¶**: PyTorch 2.0+

- **åˆ†è¯å™¨**: SentencePiece

- **æ•°æ®é›†**: [TinyStories](https://www.modelscope.cn/datasets/AI-ModelScope/TinyStories)

- **ä¼˜åŒ–å™¨**: AdamW

- **å­¦ä¹ ç‡è°ƒåº¦**: Cosine Annealing with Warmup

- **åŠ é€ŸæŠ€æœ¯**: torch.compile

### æ¨¡å‹è§„æ¨¡

- **å‚æ•°é‡**: 7.3M

- **è¯æ±‡è¡¨å¤§å°**: 4096

- **æœ€å¤§åºåˆ—é•¿åº¦**: 256

- **éšè—ç»´åº¦**: 288

- **å±‚æ•°**: 6

- **æ³¨æ„åŠ›å¤´æ•°**: 6

## ğŸ—ï¸ é¡¹ç›®æ¶æ„

### ç›®å½•ç»“æ„

```Plain Text
TinyLLM/
â”œâ”€â”€ src/                    # æºä»£ç 
â”‚   â”œâ”€â”€ config/            # é…ç½®æ–‡ä»¶
â”‚   â”‚   â””â”€â”€ config.py      # æ¨¡å‹å’Œè®­ç»ƒé…ç½®
â”‚   â””â”€â”€ checkpoints/           # æ¨¡å‹æ£€æŸ¥ç‚¹
â”‚   â”‚   â””â”€â”€ final_checkpoint.ckpt
â”‚   â”œâ”€â”€ train_vocab.py     # è®­ç»ƒtokenizer
â”‚   â”œâ”€â”€ model.py           # æ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ data_utils.py      # æ•°æ®å¤„ç†å·¥å…·
â”‚   â”œâ”€â”€ train.py           # è®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ test.py            # æ¨ç†è„šæœ¬
â”œâ”€â”€ data/                  # æ•°æ®ç›®å½•
â”‚   â””â”€â”€ TinyStories/   
â”‚       â””â”€â”€ data/
â”‚           â””â”€â”€ train-*.parquet # åŸå§‹æ•°æ®
â”‚
â”œâ”€â”€ download_data.sh
â””â”€â”€ README.md             # é¡¹ç›®è¯´æ˜
```

## ğŸ”§ é¡¹ç›®æ„å»ºè¯¦ç»†æ‹†è§£

### (1) Tokenizerçš„è®­ç»ƒ

**tokenizerä½œç”¨**ï¼šæ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—åºåˆ—

ç”±äºTinyStoriesæ•°æ®é›†çš„è¯æ±‡é‡æœ‰é™ï¼Œé‡‡ç”¨**BPEï¼ˆByte Pair Encodingï¼‰ç®—æ³•**ï¼Œ**è¯æ±‡è¡¨å¤§å°è®¾ç½®ä¸º4096**

```Shell
modelscope download --dataset 'AI-ModelScope/TinyStories' --local_dir '/home/suxin/tinyllm/data/TinyStories'
```

```Python
import os
import pyarrow.parquet as pq
from pathlib import Path
import sentencepiece as spm
import tempfile

def train_vocab(data_dir, vocab_size=4096, model_name="vocab"):
    """
    Args:
        data_dir:æ•°æ®ç›®å½•è·¯å¾„
        vocab_size:è¯æ±‡è¡¨å¤§å°
        model_name:æ¨¡å‹å
    Returns:
        æ¨¡å‹æ–‡ä»¶è·¯å¾„
    """
    print(f"å¼€å§‹è®­ç»ƒè¯æ±‡è¡¨ï¼Œè¯æ±‡è¡¨å¤§å°ï¼š {vocab_size}")

    print("Step 1: å‡†å¤‡è®­ç»ƒè¯­æ–™...")
    temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt', encoding='utf-8')
    data_path = Path(data_dir)
    parquet_files = list(data_path.glob("train-*.parquet"))
    
    text_count = 0
    for file_path in parquet_files:
        print(f"å¤„ç†æ–‡ä»¶: {file_path.name}")
        
        # è¯»å–parquetæ–‡ä»¶
        pf = pq.ParquetFile(file_path)
        for batch in pf.iter_batches(columns=["text"], batch_size=50000):
            texts = batch["text"].to_pylist()
            
            for text in texts:
                if text and len(str(text).strip()) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„æ–‡æœ¬
                    temp_file.write(str(text).strip() + '\n')
                    text_count += 1
                    
                    if text_count % 10000 == 0:
                        print(f"å·²å¤„ç† {text_count} æ¡æ–‡æœ¬")
    
    temp_file.close()
    print(f"è¯­æ–™å‡†å¤‡å®Œæˆï¼Œå…± {text_count} æ¡æ–‡æœ¬")
    
    # æ­¥éª¤2: è®­ç»ƒSentencePieceæ¨¡å‹
    print("Step 2: è®­ç»ƒSentencePieceæ¨¡å‹...")
    
    try:
        spm.SentencePieceTrainer.train(
            input=temp_file_path,                    # è¾“å…¥æ–‡ä»¶è·¯å¾„
            model_prefix=model_name,                 # è¾“å‡ºæ¨¡å‹å‰ç¼€
            vocab_size=vocab_size,                   # è¯æ±‡è¡¨å¤§å°ï¼š4096ä¸ªtoken
            model_type="bpe",                        # ç®—æ³•ç±»å‹ï¼šBPEï¼ˆå­—èŠ‚å¯¹ç¼–ç ï¼‰
            character_coverage=0.9995,               # å­—ç¬¦è¦†ç›–ç‡ï¼š99.95%çš„å­—ç¬¦è¢«åŒ…å«
            num_threads=os.cpu_count(),              # å¹¶è¡Œçº¿ç¨‹æ•°ï¼šä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
            pad_id=0,                                # å¡«å……token ID
            unk_id=1,                                # æœªçŸ¥token ID  
            bos_id=2,                                # å¥å­å¼€å§‹token ID
            eos_id=3,                                # å¥å­ç»“æŸtoken ID
            input_sentence_size=10_000_000,          # æœ€å¤§è¾“å…¥å¥å­æ•°ï¼š1000ä¸‡å¥
            shuffle_input_sentence=True              # æ‰“ä¹±è¾“å…¥å¥å­é¡ºåº
        )
        
        print(f"âœ… è¯æ±‡è¡¨è®­ç»ƒå®Œæˆ!")
        print(f"æ¨¡å‹æ–‡ä»¶: {model_name}.model")
        print(f"è¯æ±‡æ–‡ä»¶: {model_name}.vocab")
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(temp_file.name)
    
    return f"{model_name}.model"

model_path = train_vocab("/home/suxin/tinyllm/data/TinyStories/data", vocab_size=4096, model_name="TinyStories4096")
```

### (2) åˆ†è¯å™¨ç±»å®ç°

```Python
import os
import torch
import sentencepiece as spm
import pyarrow.parquet as pq
from pathlib import Path
import random

class TinyStoriesTokenizer:
    def __init__(self, model_path):
        """
        åˆå§‹åŒ–tokenizer
        Args:
            model_path: SentencePieceæ¨¡å‹æ–‡ä»¶è·¯å¾„(.modelæ–‡ä»¶)
        """
        
        self.sp = spm.SentencePieceProcessor()
        
        self.sp.load(model_path)
        print(f"âœ… æˆåŠŸåŠ è½½SentencePieceæ¨¡å‹: {model_path}")
        
        # è·å–ç‰¹æ®Štoken ID
        self.pad_id = self.sp.pad_id()
        self.unk_id = self.sp.unk_id()
        self.bos_id = self.sp.bos_id()
        self.eos_id = self.sp.eos_id()
        
        print(f"è¯æ±‡è¡¨å¤§å°: {self.sp.vocab_size()}")
        print(f"ç‰¹æ®Štoken - PAD: {self.pad_id}, UNK: {self.unk_id}, BOS: {self.bos_id}, EOS: {self.eos_id}")
    
    @property
    def vocab_size(self):
        return self.sp.vocab_size()
    
    def encode(self, text, add_bos=True, add_eos=True):
        """ç¼–ç æ–‡æœ¬"""
        if not isinstance(text, str):
            text = str(text)
        
        # åŸºç¡€ç¼–ç 
        tokens = self.sp.encode(text)
        
        # æ·»åŠ ç‰¹æ®Štoken
        if add_bos:
            tokens = [self.bos_id] + tokens
        if add_eos:
            tokens = tokens + [self.eos_id]
            
        return tokens
    
    def decode(self, tokens):
        """è§£ç token"""
        if isinstance(tokens, torch.Tensor):
            tokens = tokens.tolist()
        
        # ç§»é™¤ç‰¹æ®Štokenï¼ˆä¿ç•™å†…å®¹ä¸­çš„ç‰¹æ®Štokenï¼‰
        filtered_tokens = []
        for token in tokens:
            if token == self.eos_id:  # é‡åˆ°EOSå°±åœæ­¢
                break
            if token not in [self.pad_id, self.bos_id]:
                filtered_tokens.append(token)
        
        return self.sp.decode(filtered_tokens)
    
    def encode_batch(self, texts, add_bos=True, add_eos=True):
        """æ‰¹é‡ç¼–ç """
        return [self.encode(text, add_bos, add_eos) for text in texts]

def create_data_loader(data_dir, vocab_path, max_seq_len):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    print(f"æ­£åœ¨åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    print(f"æ•°æ®ç›®å½•: {data_dir}")
    print(f"è¯æ±‡è¡¨è·¯å¾„: {vocab_path}")
    print(f"æœ€å¤§åºåˆ—é•¿åº¦: {max_seq_len}")
    
    # åˆå§‹åŒ–tokenizer
    tokenizer = TinyStoriesTokenizer(vocab_path)
    
    # åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®
    print("åŠ è½½è®­ç»ƒæ•°æ®...")
    train_data = load_and_tokenize_data(data_dir, tokenizer, max_seq_len, split='train')
    
    print("åŠ è½½éªŒè¯æ•°æ®...")
    val_data = load_and_tokenize_data(data_dir, tokenizer, max_seq_len, split='val')
    
    def get_batch(split, batch_size):
        data = train_data if split == 'train' else val_data
        
        if len(data) < max_seq_len + 1:
            raise ValueError(f"æ•°æ®å¤ªå°‘ï¼Œæ— æ³•åˆ›å»ºæ‰¹æ¬¡ã€‚æ•°æ®é•¿åº¦: {len(data)}, éœ€è¦: {max_seq_len + 1}")
        
        # éšæœºé€‰æ‹©batch_sizeä¸ªèµ·å§‹ä½ç½®
        max_start_idx = len(data) - max_seq_len - 1
        indices = torch.randint(0, max_start_idx, (batch_size,))
        
        # åˆ›å»ºè¾“å…¥å’Œæ ‡ç­¾
        x = torch.zeros(batch_size, max_seq_len, dtype=torch.long)
        y = torch.zeros(batch_size, max_seq_len, dtype=torch.long)
        
        for i, idx in enumerate(indices):
            x[i] = torch.tensor(data[idx:idx + max_seq_len], dtype=torch.long)
            y[i] = torch.tensor(data[idx + 1:idx + max_seq_len + 1], dtype=torch.long)
        
        return x, y
    
    return get_batch, tokenizer

def load_and_tokenize_data(data_dir, tokenizer, max_seq_len, split='train', max_texts=50000):
    """åŠ è½½å¹¶åˆ†è¯æ•°æ®"""
    print(f"æ­£åœ¨åŠ è½½ {split} æ•°æ®...")
    
    data_path = Path(data_dir)
    
    # æŸ¥æ‰¾parquetæ–‡ä»¶
    train_files = list(data_path.glob("train-*.parquet"))
    val_files = list(data_path.glob("validation-*.parquet"))
    
    print(f"æ‰¾åˆ° {len(train_files)} ä¸ªè®­ç»ƒæ–‡ä»¶ï¼Œ{len(val_files)} ä¸ªéªŒè¯æ–‡ä»¶")
    
    if split == 'train':
        parquet_files = train_files
    else:
        parquet_files = val_files if val_files else train_files[-1:]  # å¦‚æœæ²¡æœ‰éªŒè¯æ–‡ä»¶ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªè®­ç»ƒæ–‡ä»¶
    
    if not parquet_files:
        raise FileNotFoundError(f"åœ¨ {data_dir} ä¸­æ‰¾ä¸åˆ°parquetæ–‡ä»¶")
    
    all_tokens = []
    text_count = 0
    
    for file_path in parquet_files:
        print(f"å¤„ç†æ–‡ä»¶: {file_path.name}")
        
        try:
            pf = pq.ParquetFile(file_path)
            for batch in pf.iter_batches(columns=["text"], batch_size=5000):
                texts = batch["text"].to_pylist()
                
                for text in texts:
                    if text and len(str(text).strip()) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„æ–‡æœ¬
                        try:
                            tokens = tokenizer.encode(str(text).strip())
                            all_tokens.extend(tokens)
                            text_count += 1
                            
                            if text_count % 1000 == 0:
                                print(f"å·²å¤„ç† {text_count} æ¡æ–‡æœ¬ï¼Œå½“å‰tokenæ•°: {len(all_tokens)}")
                            
                            if text_count >= max_texts:
                                break
                        except Exception as e:
                            print(f"ç¼–ç æ–‡æœ¬å¤±è´¥: {e}")
                            continue
                
                if text_count >= max_texts:
                    break
                
        except Exception as e:
            print(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
            continue
        
        if text_count >= max_texts:
            break
    
    if not all_tokens:
        raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®")
    
    print(f"âœ… {split} æ•°æ®åŠ è½½å®Œæˆï¼š{text_count} æ¡æ–‡æœ¬ï¼Œ{len(all_tokens)} ä¸ªtoken")
    
    # å¦‚æœæ˜¯éªŒè¯é›†ä¸”ä½¿ç”¨çš„æ˜¯è®­ç»ƒæ–‡ä»¶ï¼Œå–å20%ä½œä¸ºéªŒè¯
    if split == 'val' and not val_files:
        split_idx = int(len(all_tokens) * 0.8)
        all_tokens = all_tokens[split_idx:]
        print(f"ä½¿ç”¨è®­ç»ƒæ•°æ®çš„å20%ä½œä¸ºéªŒè¯é›†ï¼ŒéªŒè¯é›†å¤§å°: {len(all_tokens)} tokens")
    
    return all_tokens

def load_pretrained_model(model_path):
    """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡"""
    if not model_path or not os.path.exists(model_path):
        print("æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶")
        return None
    
    try:
        # æ£€æŸ¥æ˜¯å¦æ˜¯PyTorchæ£€æŸ¥ç‚¹æ–‡ä»¶
        if model_path.endswith('.model'):
            print("æ£€æµ‹åˆ°SentencePieceæ¨¡å‹æ–‡ä»¶ï¼Œè·³è¿‡æƒé‡åŠ è½½")
            return None
        
        checkpoint = torch.load(model_path, map_location='cpu')
        if 'model_state_dict' in checkpoint:
            print("ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹æƒé‡")
            return checkpoint['model_state_dict']
        else:
            print("ç›´æ¥åŠ è½½æ¨¡å‹æƒé‡")
            return checkpoint
    except Exception as e:
        print(f"åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¤±è´¥: {e}")
        return None
```

### (3) Transfomers (Decoder Only)

```Python
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, Optional
from config.config import ModelConfig

class RMSNorm(nn.Module):
    """Root Mean Square Layer Normalization
    
    å…¬å¼: x * rsqrt(mean(x^2) + eps) * weight
    """
    
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))
    
    def _norm(self, x: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—RMSå½’ä¸€åŒ–"""
        # è®¡ç®—å¹³æ–¹çš„å‡å€¼ï¼Œç„¶åå–å¹³æ–¹æ ¹çš„å€’æ•°
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # å…ˆè½¬ä¸ºfloat32è¿›è¡Œæ•°å€¼ç¨³å®šçš„è®¡ç®—ï¼Œç„¶åè½¬å›åŸå§‹ç±»å‹
        output = self._norm(x.float()).type_as(x)
        return output * self.weight
    
class RotaryEmbedding(nn.Module):
    """æ—‹è½¬ä½ç½®ç¼–ç  (RoPE)
    
    ä¸ºæŸ¥è¯¢å’Œé”®å‘é‡æ·»åŠ ä½ç½®ä¿¡æ¯ï¼Œé€šè¿‡æ—‹è½¬çš„æ–¹å¼ç¼–ç ç›¸å¯¹ä½ç½®
    """
    
    def __init__(self, dim: int, max_seq_len: int = 2048, theta: float = 10000.0):
        super().__init__()
        self.dim = dim
        self.max_seq_len = max_seq_len
        self.theta = theta
        
        # é¢„è®¡ç®—æ—‹è½¬é¢‘ç‡
        self._build_cache(max_seq_len)
    
    def _build_cache(self, max_seq_len: int):
        """æ„å»ºæ—‹è½¬é¢‘ç‡ç¼“å­˜"""
        # è®¡ç®—é¢‘ç‡ 1/theta^(2i/d)
        freqs = 1.0 / (self.theta ** (torch.arange(0, self.dim, 2).float() / self.dim))
        
        # ç”Ÿæˆä½ç½®åºåˆ—
        t = torch.arange(max_seq_len).float()
        
        # è®¡ç®—ä½ç½®ä¸é¢‘ç‡çš„å¤–ç§¯
        freqs = torch.outer(t, freqs)
        
        # è®¡ç®—coså’Œsin
        freqs_cos = torch.cos(freqs)
        freqs_sin = torch.sin(freqs)                               
        
        # æ³¨å†Œä¸ºç¼“å†²åŒºï¼ˆä¸å‚ä¸æ¢¯åº¦è®¡ç®—ï¼‰
        self.register_buffer("freqs_cos", freqs_cos, persistent=False)
        self.register_buffer("freqs_sin", freqs_sin, persistent=False)
    
    def forward(self, x: torch.Tensor, seq_len: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """è¿”å›å¯¹åº”åºåˆ—é•¿åº¦çš„coså’Œsinå€¼"""
        return self.freqs_cos[:seq_len], self.freqs_sin[:seq_len]
    
def apply_rotary_pos_emb(
    q: torch.Tensor, 
    k: torch.Tensor, 
    cos: torch.Tensor, 
    sin: torch.Tensor
) -> Tuple[torch.Tensor, torch.Tensor]:
    """åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç åˆ°æŸ¥è¯¢å’Œé”®å‘é‡
    
    Args:
        q: æŸ¥è¯¢å‘é‡ [batch, seq_len, n_heads, head_dim]
        k: é”®å‘é‡ [batch, seq_len, n_kv_heads, head_dim]
        cos: ä½™å¼¦å€¼ [seq_len, head_dim]
        sin: æ­£å¼¦å€¼ [seq_len, head_dim]
    
    Returns:
        æ—‹è½¬åçš„æŸ¥è¯¢å’Œé”®å‘é‡
    """
    # é‡æ–°è°ƒæ•´coså’Œsinçš„ç»´åº¦ä»¥åŒ¹é…qå’Œk
    cos = cos.view(1, cos.shape[0], 1, cos.shape[1])  # [1, seq_len, 1, head_dim]
    sin = sin.view(1, sin.shape[0], 1, sin.shape[1])
    
    # å°†æœ€åä¸€ä¸ªç»´åº¦åˆ†æˆä¸¤éƒ¨åˆ†ï¼ˆå®éƒ¨å’Œè™šéƒ¨ï¼‰
    q_r, q_i = q.chunk(2, dim=-1)
    k_r, k_i = k.chunk(2, dim=-1)
    
    # åº”ç”¨æ—‹è½¬å˜æ¢
    q_out = torch.cat([
        q_r * cos - q_i * sin,
        q_r * sin + q_i * cos
    ], dim=-1)
    
    k_out = torch.cat([
        k_r * cos - k_i * sin,
        k_r * sin + k_i * cos
    ], dim=-1)
    
    return q_out, k_out

def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
    """é‡å¤é”®å€¼å‘é‡ä»¥åŒ¹é…æŸ¥è¯¢å¤´æ•°ï¼ˆç”¨äºGrouped Query Attentionï¼‰
    
    Args:
        x: é”®æˆ–å€¼å‘é‡ [batch, seq_len, n_kv_heads, head_dim]
        n_rep: é‡å¤æ¬¡æ•°
    
    Returns:
        é‡å¤åçš„å‘é‡ [batch, seq_len, n_heads, head_dim]
    """
    if n_rep == 1:
        return x
    
    batch, seq_len, n_kv_heads, head_dim = x.shape
    
    # åœ¨ç¬¬4ä¸ªç»´åº¦æ’å…¥é‡å¤
    x = x[:, :, :, None, :].expand(batch, seq_len, n_kv_heads, n_rep, head_dim)
    
    # é‡æ–°è°ƒæ•´å½¢çŠ¶
    return x.reshape(batch, seq_len, n_kv_heads * n_rep, head_dim)

class MLP(nn.Module):
    """å¤šå±‚æ„ŸçŸ¥æœº (MLP)
    
    ä½¿ç”¨SwiGLUæ¿€æ´»å‡½æ•°: SwiGLU(x) = Swish(W1*x) * (W3*x)
    å…¶ä¸­ Swish(x) = x * sigmoid(x) = x * SiLU(x)
    """
    
    def __init__(self, dim: int, hidden_dim: int, dropout: float = 0.0):
        super().__init__()
        
        # ä¸‰ä¸ªçº¿æ€§å±‚
        self.gate_proj = nn.Linear(dim, hidden_dim, bias=False)  # W1
        self.down_proj = nn.Linear(hidden_dim, dim, bias=False)  # W2
        self.up_proj = nn.Linear(dim, hidden_dim, bias=False)    # W3
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­: SwiGLU(x) = SiLU(W1*x) * (W3*x)"""
        gate = F.silu(self.gate_proj(x))  # SiLUæ¿€æ´»
        up = self.up_proj(x)              # çº¿æ€§å˜æ¢
        out = gate * up                   # é—¨æ§æœºåˆ¶
        return self.dropout(self.down_proj(out))
    
class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
    
    æ”¯æŒï¼š
    - æ ‡å‡†å¤šå¤´æ³¨æ„åŠ› (MHA)
    - åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ› (GQA) 
    - å¤šæŸ¥è¯¢æ³¨æ„åŠ› (MQA)
    """
    
    def __init__(self, config):
        super().__init__()
        
        # åŸºç¡€é…ç½®
        self.dim = config.dim
        self.n_heads = config.n_heads
        self.n_kv_heads = config.n_kv_heads
        self.head_dim = self.dim // self.n_heads
        self.dropout = config.dropout
        
        # è®¡ç®—é‡å¤æ¬¡æ•°ï¼ˆç”¨äºGQAï¼‰
        assert self.n_heads % self.n_kv_heads == 0
        self.n_rep = self.n_heads // self.n_kv_heads
        
        # æŠ•å½±å±‚
        self.q_proj = nn.Linear(self.dim, self.n_heads * self.head_dim, bias=False)
        self.k_proj = nn.Linear(self.dim, self.n_kv_heads * self.head_dim, bias=False)
        self.v_proj = nn.Linear(self.dim, self.n_kv_heads * self.head_dim, bias=False)
        self.o_proj = nn.Linear(self.n_heads * self.head_dim, self.dim, bias=False)
        
        # Dropout
        self.attn_dropout = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)
        
        # æ£€æŸ¥æ˜¯å¦æ”¯æŒFlash Attention
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')
        
        if not self.flash:
            print("Warning: Flash Attentionä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†å®ç°")
            # åˆ›å»ºå› æœæ©ç 
            self.register_buffer(
                "causal_mask",
                torch.triu(torch.ones(config.max_seq_len, config.max_seq_len), diagonal=1).bool(),
                persistent=False
            )
    
    def forward(
        self, 
        x: torch.Tensor, 
        freqs_cos: torch.Tensor, 
        freqs_sin: torch.Tensor,
        mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Args:
            x: è¾“å…¥å¼ é‡ [batch, seq_len, dim]
            freqs_cos: æ—‹è½¬ç¼–ç coså€¼ [seq_len, head_dim]
            freqs_sin: æ—‹è½¬ç¼–ç sinå€¼ [seq_len, head_dim]
            mask: å¯é€‰çš„æ³¨æ„åŠ›æ©ç 
        
        Returns:
            è¾“å‡ºå¼ é‡ [batch, seq_len, dim]
        """
        batch_size, seq_len, _ = x.shape
        
        # 1. æŠ•å½±å¾—åˆ°Q, K, V
        q = self.q_proj(x)  # [batch, seq_len, n_heads * head_dim]
        k = self.k_proj(x)  # [batch, seq_len, n_kv_heads * head_dim]
        v = self.v_proj(x)  # [batch, seq_len, n_kv_heads * head_dim]
        
        # 2. é‡å¡‘ä¸ºå¤šå¤´å½¢å¼
        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim)
        k = k.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)
        v = v.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)
        
        # 3. åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç 
        q, k = apply_rotary_pos_emb(q, k, freqs_cos, freqs_sin)
        
        # 4. ä¸ºGQAé‡å¤K, V
        if self.n_rep > 1:
            k = repeat_kv(k, self.n_rep)
            v = repeat_kv(v, self.n_rep)
        
        # 5. è½¬ç½®ä¸ºæ³¨æ„åŠ›è®¡ç®—çš„å½¢å¼ [batch, n_heads, seq_len, head_dim]
        q = q.transpose(1, 2)
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)
        
        # 6. è®¡ç®—æ³¨æ„åŠ›
        if self.flash:
            # ä½¿ç”¨Flash Attention
            attn_output = F.scaled_dot_product_attention(
                q, k, v,
                attn_mask=mask,
                dropout_p=self.dropout if self.training else 0.0,
                is_causal=True if mask is None else False
            )
        else:
            # æ‰‹åŠ¨å®ç°æ³¨æ„åŠ›
            attn_output = self._manual_attention(q, k, v, mask, seq_len)
        
        # 7. é‡å¡‘è¾“å‡º
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.view(batch_size, seq_len, self.n_heads * self.head_dim)
        
        # 8. è¾“å‡ºæŠ•å½±
        output = self.o_proj(attn_output)
        output = self.resid_dropout(output)
        
        return output
    
    def _manual_attention(
        self, 
        q: torch.Tensor, 
        k: torch.Tensor, 
        v: torch.Tensor, 
        mask: Optional[torch.Tensor],
        seq_len: int
    ) -> torch.Tensor:
        """æ‰‹åŠ¨å®ç°çš„æ³¨æ„åŠ›è®¡ç®—"""
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        # åº”ç”¨å› æœæ©ç 
        if mask is None:
            # ä½¿ç”¨é¢„è®¾çš„å› æœæ©ç 
            causal_mask = self.causal_mask[:seq_len, :seq_len]
            scores = scores.masked_fill(causal_mask, float('-inf'))
        else:
            # ä½¿ç”¨è‡ªå®šä¹‰æ©ç 
            scores = scores.masked_fill(mask, float('-inf'))
        
        # Softmax
        attn_weights = F.softmax(scores, dim=-1, dtype=torch.float32).type_as(q)
        attn_weights = self.attn_dropout(attn_weights)
        
        # åŠ æƒæ±‚å’Œ
        attn_output = torch.matmul(attn_weights, v)
        
        return attn_output
    
class KVCache:
    """é”®å€¼ç¼“å­˜ï¼Œç”¨äºæ¨ç†åŠ é€Ÿ"""
    
    def __init__(self, max_batch_size: int, max_seq_len: int, n_kv_heads: int, head_dim: int, dtype=torch.float16):
        self.max_batch_size = max_batch_size
        self.max_seq_len = max_seq_len
        self.n_kv_heads = n_kv_heads
        self.head_dim = head_dim
        self.dtype = dtype
        
        # åˆå§‹åŒ–ç¼“å­˜
        self.cache_k = torch.zeros(
            (max_batch_size, max_seq_len, n_kv_heads, head_dim),
            dtype=dtype
        )
        self.cache_v = torch.zeros(
            (max_batch_size, max_seq_len, n_kv_heads, head_dim),
            dtype=dtype
        )
        self.cache_len = 0
    
    def update(self, k: torch.Tensor, v: torch.Tensor, start_pos: int = 0):
        """æ›´æ–°ç¼“å­˜"""
        batch_size, seq_len, n_kv_heads, head_dim = k.shape
        
        # ç¡®ä¿ç¼“å­˜åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if self.cache_k.device != k.device:
            self.cache_k = self.cache_k.to(k.device)
            self.cache_v = self.cache_v.to(k.device)
        
        # æ›´æ–°ç¼“å­˜
        self.cache_k[:batch_size, start_pos:start_pos + seq_len] = k
        self.cache_v[:batch_size, start_pos:start_pos + seq_len] = v
        self.cache_len = max(self.cache_len, start_pos + seq_len)
    
    def get(self, batch_size: int, seq_len: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """è·å–ç¼“å­˜çš„é”®å€¼"""
        return (
            self.cache_k[:batch_size, :seq_len],
            self.cache_v[:batch_size, :seq_len]
        )
    
    def reset(self):
        """é‡ç½®ç¼“å­˜"""
        self.cache_len = 0

class CachedMultiHeadAttention(MultiHeadAttention):
    """æ”¯æŒKVç¼“å­˜çš„å¤šå¤´æ³¨æ„åŠ›"""
    
    def __init__(self, config):
        super().__init__(config)
        self.kv_cache = None
    
    def setup_cache(self, max_batch_size: int, max_seq_len: int, dtype=torch.float16):
        """è®¾ç½®KVç¼“å­˜"""
        self.kv_cache = KVCache(
            max_batch_size=max_batch_size,
            max_seq_len=max_seq_len,
            n_kv_heads=self.n_kv_heads,
            head_dim=self.head_dim,
            dtype=dtype
        )
    
    def forward_with_cache(
        self,
        x: torch.Tensor,
        freqs_cos: torch.Tensor,
        freqs_sin: torch.Tensor,
        start_pos: int = 0,
        mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """å¸¦ç¼“å­˜çš„å‰å‘ä¼ æ’­"""
        batch_size, seq_len, _ = x.shape
        
        # æŠ•å½±å¾—åˆ°Q, K, V
        q = self.q_proj(x)
        k = self.k_proj(x)
        v = self.v_proj(x)
        
        # é‡å¡‘
        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim)
        k = k.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)
        v = v.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)
        
        # åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç 
        q, k = apply_rotary_pos_emb(q, k, freqs_cos, freqs_sin)
        
        # æ›´æ–°ç¼“å­˜
        if self.kv_cache is not None:
            self.kv_cache.update(k, v, start_pos)
            # è·å–å®Œæ•´çš„é”®å€¼åºåˆ—
            k, v = self.kv_cache.get(batch_size, start_pos + seq_len)
        
        # ä¸ºGQAé‡å¤K, V
        if self.n_rep > 1:
            k = repeat_kv(k, self.n_rep)
            v = repeat_kv(v, self.n_rep)
        
        # è½¬ç½®
        q = q.transpose(1, 2)
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)
        
        # è®¡ç®—æ³¨æ„åŠ›ï¼ˆåªå…³æ³¨åˆ°å½“å‰ä½ç½®ï¼‰
        if self.flash:
            attn_output = F.scaled_dot_product_attention(
                q, k, v,
                attn_mask=mask,
                dropout_p=0.0,  # æ¨ç†æ—¶ä¸ä½¿ç”¨dropout
                is_causal=True if mask is None else False
            )
        else:
            attn_output = self._manual_attention_cached(q, k, v, start_pos, seq_len)
        
        # é‡å¡‘è¾“å‡º
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.view(batch_size, seq_len, self.n_heads * self.head_dim)
        
        # è¾“å‡ºæŠ•å½±
        output = self.o_proj(attn_output)
        
        return output
    
    def _manual_attention_cached(
        self, 
        q: torch.Tensor, 
        k: torch.Tensor, 
        v: torch.Tensor, 
        start_pos: int,
        seq_len: int
    ) -> torch.Tensor:
        """ç¼“å­˜æ¨¡å¼çš„æ‰‹åŠ¨æ³¨æ„åŠ›è®¡ç®—"""
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        # åº”ç”¨å› æœæ©ç  - åªèƒ½çœ‹åˆ°å½“å‰ä½ç½®åŠä¹‹å‰çš„ä½ç½®
        total_len = start_pos + seq_len
        if hasattr(self, 'causal_mask'):
            causal_mask = self.causal_mask[start_pos:total_len, :total_len]
            scores = scores.masked_fill(causal_mask, float('-inf'))
        
        # Softmax
        attn_weights = F.softmax(scores, dim=-1, dtype=torch.float32).type_as(q)
        
        # åŠ æƒæ±‚å’Œ
        attn_output = torch.matmul(attn_weights, v)
        
        return attn_output    
    
class TransformerBlock(nn.Module):
    """Transformerè§£ç å™¨å—
    
    ç»“æ„ï¼š
    x -> RMSNorm -> Attention -> Add -> RMSNorm -> MLP -> Add -> output
    """
    
    def __init__(self, layer_id: int, config):
        super().__init__()
        self.layer_id = layer_id
        self.dim = config.dim
        
        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = MultiHeadAttention(config)
        
        # MLP
        self.mlp = MLP(
            dim=config.dim,
            hidden_dim=config.hidden_dim,
            dropout=config.dropout
        )
        
        # å½’ä¸€åŒ–å±‚
        self.attention_norm = RMSNorm(config.dim, eps=config.norm_eps)
        self.ffn_norm = RMSNorm(config.dim, eps=config.norm_eps)
    
    def forward(
        self,
        x: torch.Tensor,
        freqs_cos: torch.Tensor,
        freqs_sin: torch.Tensor,
        mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Args:
            x: è¾“å…¥å¼ é‡ [batch, seq_len, dim]
            freqs_cos: æ—‹è½¬ç¼–ç coså€¼ [seq_len, head_dim]
            freqs_sin: æ—‹è½¬ç¼–ç sinå€¼ [seq_len, head_dim]
            mask: å¯é€‰çš„æ³¨æ„åŠ›æ©ç 
        
        Returns:
            è¾“å‡ºå¼ é‡ [batch, seq_len, dim]
        """
        # 1. è‡ªæ³¨æ„åŠ›å­å±‚ (Pre-Norm)
        # x + Attention(RMSNorm(x))
        h = x + self.attention(
            self.attention_norm(x),
            freqs_cos,
            freqs_sin,
            mask
        )
        
        # 2. å‰é¦ˆç½‘ç»œå­å±‚ (Pre-Norm)
        # h + MLP(RMSNorm(h))
        out = h + self.mlp(self.ffn_norm(h))
        
        return out
    
class CachedTransformerBlock(nn.Module):
    """æ”¯æŒKVç¼“å­˜çš„Transformerè§£ç å™¨å—"""
    
    def __init__(self, layer_id: int, config):
        super().__init__()
        self.layer_id = layer_id
        self.dim = config.dim
        
        # ç¼“å­˜æ³¨æ„åŠ›æœºåˆ¶
        self.attention = CachedMultiHeadAttention(config)
        
        # MLP
        self.mlp = MLP(
            dim=config.dim,
            hidden_dim=config.hidden_dim,
            dropout=config.dropout
        )
        
        # å½’ä¸€åŒ–å±‚
        self.attention_norm = RMSNorm(config.dim, eps=config.norm_eps)
        self.ffn_norm = RMSNorm(config.dim, eps=config.norm_eps)
    
    def setup_cache(self, max_batch_size: int, max_seq_len: int, dtype=torch.float16):
        """è®¾ç½®KVç¼“å­˜"""
        self.attention.setup_cache(max_batch_size, max_seq_len, dtype)
    
    def forward_with_cache(
        self,
        x: torch.Tensor,
        freqs_cos: torch.Tensor,
        freqs_sin: torch.Tensor,
        start_pos: int = 0,
        mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """å¸¦ç¼“å­˜çš„å‰å‘ä¼ æ’­"""
        # è‡ªæ³¨æ„åŠ›å­å±‚
        h = x + self.attention.forward_with_cache(
            self.attention_norm(x),
            freqs_cos,
            freqs_sin,
            start_pos,
            mask
        )
        
        # å‰é¦ˆç½‘ç»œå­å±‚
        out = h + self.mlp(self.ffn_norm(h))
        
        return out
    
class Transformer(nn.Module):
    """å®Œæ•´çš„Transformerè¯­è¨€æ¨¡å‹
    
    æ”¯æŒï¼š
    - è®­ç»ƒæ¨¡å¼ï¼šå®Œæ•´çš„åºåˆ—åˆ°åºåˆ—å¤„ç†
    - æ¨ç†æ¨¡å¼ï¼šå¢é‡ç”Ÿæˆï¼Œæ”¯æŒKVç¼“å­˜
    """
    
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.config = config
        self.vocab_size = config.vocab_size
        self.n_layers = config.n_layers
        self.dim = config.dim
        self.max_seq_len = config.max_seq_len
        
        # === æ ¸å¿ƒç»„ä»¶ ===
        # 1. è¯åµŒå…¥å±‚
        self.tok_embeddings = nn.Embedding(config.vocab_size, config.dim)
        
        # 2. Dropoutå±‚
        self.dropout = nn.Dropout(config.dropout)
        
        # 3. æ—‹è½¬ä½ç½®ç¼–ç 
        self.rope = RotaryEmbedding(
            dim=config.dim // config.n_heads,
            max_seq_len=config.max_seq_len,
            theta=config.rope_theta
        )
        
        # 4. Transformerå±‚
        self.layers = nn.ModuleList([
            TransformerBlock(layer_id, config)
            for layer_id in range(config.n_layers)
        ])
        
        # 5. æœ€ç»ˆå½’ä¸€åŒ–å±‚
        self.norm = RMSNorm(config.dim, eps=config.norm_eps)
        
        # 6. è¾“å‡ºæŠ•å½±å±‚
        self.output = nn.Linear(config.dim, config.vocab_size, bias=False)
        
        # === æƒé‡å…±äº« ===
        # è¾“å…¥åµŒå…¥å’Œè¾“å‡ºæŠ•å½±å…±äº«æƒé‡
        self.tok_embeddings.weight = self.output.weight
        
        # === åˆå§‹åŒ– ===
        self.apply(self._init_weights)
        
        # å¯¹ç‰¹å®šå±‚åº”ç”¨ç¼©æ”¾åˆå§‹åŒ–
        for name, param in self.named_parameters():
            if name.endswith('o_proj.weight') or name.endswith('down_proj.weight'):
                # å¯¹è¾“å‡ºæŠ•å½±å±‚åº”ç”¨ç¼©æ”¾ï¼Œæœ‰åŠ©äºè®­ç»ƒç¨³å®šæ€§
                nn.init.normal_(param, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layers))
        
        # è®­ç»ƒçŠ¶æ€
        self.last_loss = None
        
        # æ¨ç†çŠ¶æ€
        self.inference_mode = False
        self.cached_layers = None
    
    def _init_weights(self, module):
        """æƒé‡åˆå§‹åŒ–"""
        if isinstance(module, nn.Linear):
            # çº¿æ€§å±‚ä½¿ç”¨æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            # åµŒå…¥å±‚ä½¿ç”¨æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
    
    def setup_inference_cache(self, max_batch_size: int, max_seq_len: int, dtype=torch.float16):
        """è®¾ç½®æ¨ç†ç¼“å­˜"""
        self.cached_layers = nn.ModuleList([
            CachedTransformerBlock(layer_id, self.config)
            for layer_id in range(self.config.n_layers)
        ])
        
        # å¤åˆ¶è®­ç»ƒæƒé‡åˆ°ç¼“å­˜å±‚
        for cached_layer, train_layer in zip(self.cached_layers, self.layers):
            cached_layer.load_state_dict(train_layer.state_dict())
            cached_layer.setup_cache(max_batch_size, max_seq_len, dtype)
        
        self.inference_mode = True
        print(f"æ¨ç†ç¼“å­˜è®¾ç½®å®Œæˆ: max_batch_size={max_batch_size}, max_seq_len={max_seq_len}")
    
    def forward(
        self, 
        tokens: torch.Tensor, 
        targets: Optional[torch.Tensor] = None,
        mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """è®­ç»ƒæ¨¡å¼çš„å‰å‘ä¼ æ’­
        
        Args:
            tokens: è¾“å…¥token [batch, seq_len]
            targets: ç›®æ ‡token [batch, seq_len]ï¼Œç”¨äºè®¡ç®—æŸå¤±
            mask: å¯é€‰çš„æ³¨æ„åŠ›æ©ç 
        
        Returns:
            logits: [batch, seq_len, vocab_size] æˆ– [batch, 1, vocab_size]
        """
        batch_size, seq_len = tokens.shape
        
        # 1. è¯åµŒå…¥
        h = self.tok_embeddings(tokens)
        h = self.dropout(h)
        
        # 2. è·å–æ—‹è½¬ä½ç½®ç¼–ç 
        freqs_cos, freqs_sin = self.rope(h, seq_len)
        
        # 3. é€šè¿‡Transformerå±‚
        for layer in self.layers:
            h = layer(h, freqs_cos, freqs_sin, mask)
        
        # 4. æœ€ç»ˆå½’ä¸€åŒ–
        h = self.norm(h)
        
        # 5. è¾“å‡ºæŠ•å½±
        if targets is not None:
            # è®­ç»ƒæ¨¡å¼ï¼šè®¡ç®—æ‰€æœ‰ä½ç½®çš„logits
            logits = self.output(h)
            # è®¡ç®—æŸå¤±
            self.last_loss = F.cross_entropy(
                logits.view(-1, logits.size(-1)), 
                targets.view(-1), 
                ignore_index=-1
            )
        else:
            # æ¨ç†æ¨¡å¼ï¼šåªè®¡ç®—æœ€åä¸€ä¸ªä½ç½®çš„logits
            logits = self.output(h[:, [-1], :])
            self.last_loss = None
        
        return logits
    
    def forward_with_cache(
        self,
        tokens: torch.Tensor,
        start_pos: int = 0
    ) -> torch.Tensor:
        """ä½¿ç”¨KVç¼“å­˜çš„å‰å‘ä¼ æ’­ï¼ˆæ¨ç†ä¸“ç”¨ï¼‰"""
        if not self.inference_mode or self.cached_layers is None:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨setup_inference_cache()è®¾ç½®æ¨ç†ç¼“å­˜")
        
        batch_size, seq_len = tokens.shape
        
        # 1. è¯åµŒå…¥
        h = self.tok_embeddings(tokens)
        
        # 2. è·å–æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆä»start_poså¼€å§‹ï¼‰
        total_len = start_pos + seq_len
        freqs_cos, freqs_sin = self.rope(h, total_len)
        
        # åªå–å½“å‰åºåˆ—å¯¹åº”çš„ç¼–ç 
        freqs_cos = freqs_cos[start_pos:total_len]
        freqs_sin = freqs_sin[start_pos:total_len]
        
        # 3. é€šè¿‡ç¼“å­˜çš„Transformerå±‚
        for layer in self.cached_layers:
            h = layer.forward_with_cache(h, freqs_cos, freqs_sin, start_pos)
        
        # 4. æœ€ç»ˆå½’ä¸€åŒ–å’Œè¾“å‡º
        h = self.norm(h)
        logits = self.output(h)
        
        return logits
    
    @torch.inference_mode()
    def generate(
        self,
        prompt_tokens: torch.Tensor,
        max_new_tokens: int = 100,
        temperature: float = 1.0,
        top_k: Optional[int] = None,
        top_p: Optional[float] = None,
        do_sample: bool = True,
        use_cache: bool = True
    ) -> torch.Tensor:
        """æ–‡æœ¬ç”Ÿæˆ
        
        Args:
            prompt_tokens: æç¤ºtoken [batch, prompt_len]
            max_new_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: é‡‡æ ·æ¸©åº¦
            top_k: top-ké‡‡æ ·
            top_p: nucleusé‡‡æ ·
            do_sample: æ˜¯å¦é‡‡æ ·ï¼ŒFalseåˆ™ä½¿ç”¨è´ªå¿ƒè§£ç 
            use_cache: æ˜¯å¦ä½¿ç”¨KVç¼“å­˜
        
        Returns:
            generated_tokens: [batch, prompt_len + max_new_tokens]
        """
        batch_size, prompt_len = prompt_tokens.shape
        
        # æ£€æŸ¥åºåˆ—é•¿åº¦
        if prompt_len + max_new_tokens > self.max_seq_len:
            print(f"Warning: æ€»é•¿åº¦({prompt_len + max_new_tokens})è¶…è¿‡æœ€å¤§é•¿åº¦({self.max_seq_len})")
        
        # åˆå§‹åŒ–ç”Ÿæˆåºåˆ—
        generated = prompt_tokens.clone()
        
        if use_cache and self.inference_mode:
            # ä½¿ç”¨ç¼“å­˜çš„æ¨ç†
            return self._generate_with_cache(
                prompt_tokens, max_new_tokens, temperature, top_k, top_p, do_sample
            )
        else:
            # ä¸ä½¿ç”¨ç¼“å­˜çš„æ¨ç†
            return self._generate_without_cache(
                prompt_tokens, max_new_tokens, temperature, top_k, top_p, do_sample
            )
    
    def _generate_with_cache(
        self,
        prompt_tokens: torch.Tensor,
        max_new_tokens: int,
        temperature: float,
        top_k: Optional[int],
        top_p: Optional[float],
        do_sample: bool
    ) -> torch.Tensor:
        """ä½¿ç”¨KVç¼“å­˜çš„ç”Ÿæˆ"""
        batch_size, prompt_len = prompt_tokens.shape
        generated = prompt_tokens.clone()
        
        # 1. å¤„ç†æç¤ºåºåˆ—
        logits = self.forward_with_cache(prompt_tokens, start_pos=0)
        next_token = self._sample_next_token(
            logits[:, -1, :], temperature, top_k, top_p, do_sample
        )
        generated = torch.cat([generated, next_token], dim=1)
        
        # 2. é€ä¸ªç”Ÿæˆæ–°token
        for i in range(max_new_tokens - 1):
            logits = self.forward_with_cache(next_token, start_pos=prompt_len + i)
            next_token = self._sample_next_token(
                logits[:, -1, :], temperature, top_k, top_p, do_sample
            )
            generated = torch.cat([generated, next_token], dim=1)
        
        return generated
    
    def _generate_without_cache(
        self,
        prompt_tokens: torch.Tensor,
        max_new_tokens: int,
        temperature: float,
        top_k: Optional[int],
        top_p: Optional[float],
        do_sample: bool
    ) -> torch.Tensor:
        """ä¸ä½¿ç”¨KVç¼“å­˜çš„ç”Ÿæˆ"""
        generated = prompt_tokens.clone()
        
        for _ in range(max_new_tokens):
            # å‰å‘ä¼ æ’­æ•´ä¸ªåºåˆ—
            logits = self.forward(generated)
            
            # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
            next_token = self._sample_next_token(
                logits[:, -1, :], temperature, top_k, top_p, do_sample
            )
            
            # æ·»åŠ åˆ°ç”Ÿæˆåºåˆ—
            generated = torch.cat([generated, next_token], dim=1)
            
            # æ£€æŸ¥é•¿åº¦é™åˆ¶
            if generated.shape[1] >= self.max_seq_len:
                break
        
        return generated
    
    def _sample_next_token(
        self,
        logits: torch.Tensor,
        temperature: float,
        top_k: Optional[int],
        top_p: Optional[float],
        do_sample: bool
    ) -> torch.Tensor:
        """é‡‡æ ·ä¸‹ä¸€ä¸ªtoken"""
        if not do_sample:
            # è´ªå¿ƒè§£ç 
            return torch.argmax(logits, dim=-1, keepdim=True)
        
        # åº”ç”¨æ¸©åº¦
        if temperature > 0:
            logits = logits / temperature
        
        # Top-k é‡‡æ ·
        if top_k is not None:
            top_k = min(top_k, logits.size(-1))
            # æ‰¾åˆ°top-kçš„æœ€å°å€¼ï¼Œå°†å…¶ä»–å€¼è®¾ä¸º-inf
            top_k_values, _ = torch.topk(logits, top_k, dim=-1)
            min_top_k = top_k_values[:, -1].unsqueeze(-1)
            logits = torch.where(logits < min_top_k, 
                               torch.tensor(float('-inf'), device=logits.device),
                               logits)
        
        # Top-p (nucleus) é‡‡æ ·
        if top_p is not None:
            sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)
            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
            
            # æ‰¾åˆ°ç´¯ç§¯æ¦‚ç‡è¶…è¿‡top_pçš„ä½ç½®
            sorted_indices_to_remove = cumulative_probs > top_p
            # ä¿ç•™ç¬¬ä¸€ä¸ªè¶…è¿‡top_pçš„token
            sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()
            sorted_indices_to_remove[:, 0] = 0
            
            # åœ¨åŸå§‹logitsä¸Šåº”ç”¨æ©ç 
            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
            logits = logits.masked_fill(indices_to_remove, float('-inf'))
        
        # é‡‡æ ·
        probs = F.softmax(logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)
        
        return next_token
    
    def get_num_params(self, non_embedding=True):
        """è·å–å‚æ•°æ•°é‡"""
        n_params = sum(p.numel() for p in self.parameters())
        if non_embedding:
            n_params -= self.tok_embeddings.weight.numel()
        return n_params
    
    def estimate_mfu(self, fwdbwd_per_iter, dt):
        """ä¼°è®¡æ¨¡å‹æµ®ç‚¹è¿ç®—åˆ©ç”¨ç‡ (MFU)"""
        # é¦–å…ˆä¼°è®¡æ¯æ¬¡å‰å‘ä¼ æ’­çš„æµ®ç‚¹è¿ç®—æ•°
        N = self.get_num_params()
        cfg = self.config
        L, H, Q, T = cfg.n_layers, cfg.n_heads, cfg.dim//cfg.n_heads, cfg.max_seq_len
        flops_per_token = 6*N + 12*L*H*Q*T
        flops_per_fwdbwd = flops_per_token * T
        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter
        # è¡¨è¾¾ä¸ºæ¯ç§’æµ®ç‚¹è¿ç®—æ•°
        flops_achieved = flops_per_iter * (1.0/dt)
        # ç¡¬ä»¶çš„å³°å€¼æµ®ç‚¹è¿ç®—æ•°ï¼Œå¯¹äºA100çº¦ä¸º312 TFLOPS
        flops_promised = 312e12
        mfu = flops_achieved / flops_promised
        return mfu
    
    @classmethod
    def from_pretrained(cls, model_path: str):
        """ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½"""
        checkpoint = torch.load(model_path, map_location='cpu')
        
        # ä»checkpointä¸­æ¢å¤é…ç½®
        config = ModelConfig(**checkpoint['config'])
        
        # åˆ›å»ºæ¨¡å‹
        model = cls(config)
        
        # åŠ è½½æƒé‡
        model.load_state_dict(checkpoint['model'])
        
        return model
    
    def save_checkpoint(self, path: str, optimizer=None, iter_num=None, best_val_loss=None):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'model': self.state_dict(),
            'config': self.config.__dict__,
            'iter_num': iter_num,
            'best_val_loss': best_val_loss,
        }
        
        if optimizer is not None:
            checkpoint['optimizer'] = optimizer.state_dict()
        
        torch.save(checkpoint, path)
        print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {path}")

```



![image.png](TinyLLM/image.png)

### (4) train.py

```Python
import os
import time
import math
import torch
import torch.nn as nn
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, destroy_process_group
import swanlab
from config.config import ModelConfig
from model import Transformer
from data_utils import create_data_loader, load_pretrained_model


class TrainerConfig:
    """è®­ç»ƒé…ç½®"""
    # æ•°æ®
    dataset = 'tinystories'
    batch_size = 32
    max_seq_len = 256
    
    # è®­ç»ƒ
    max_iters = 50000
    eval_interval = 500
    eval_iters = 100
    log_interval = 100
    
    # ä¼˜åŒ–å™¨
    learning_rate = 5e-4
    weight_decay = 1e-1
    beta1 = 0.9
    beta2 = 0.95
    grad_clip = 1.0
    
    # å­¦ä¹ ç‡è°ƒåº¦
    decay_lr = True
    warmup_iters = 1000
    lr_decay_iters = 5000
    min_lr = 5e-5
    
    # ç³»ç»Ÿ
    device = 'cuda:4' if torch.cuda.is_available() else 'cpu'
    dtype = torch.float16 if torch.cuda.is_available() else torch.float32
    compile = True
    
    # ä¿å­˜
    always_save_checkpoint = False
    
    # åˆ†å¸ƒå¼è®­ç»ƒ
    backend = 'nccl'
    
    # æ–‡ä»¶è·¯å¾„
    data_dir = '/home/suxin/tinyllm/data/TinyStories/data'
    vocab_path = '/home/suxin/tinyllm/src/TinyStories4096.model'
    pretrained_model = '/home/suxin/tinyllm/src/TinyStories4096.model'

    # ç›‘æ§é…ç½®
    swanlab_project = "TinyStories"
    log_interval = 10
    out_dir = "checkpoints"
    use_swanlab: bool = False

class Trainer:
    """è®­ç»ƒå™¨"""
    
    def __init__(self, model_config: ModelConfig, train_config: TrainerConfig):
        self.model_config = model_config
        self.train_config = train_config
        
        # è®¾ç½®è®¾å¤‡
        self.device = train_config.device
        self.dtype = train_config.dtype
        
        # åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®
        self.ddp = int(os.environ.get('RANK', -1)) != -1
        if self.ddp:
            init_process_group(backend=train_config.backend)
            self.ddp_rank = int(os.environ['RANK'])
            self.ddp_local_rank = int(os.environ['LOCAL_RANK'])
            self.ddp_world_size = int(os.environ['WORLD_SIZE'])
            self.device = f'cuda:{self.ddp_local_rank}'
            torch.cuda.set_device(self.device)
            self.master_process = self.ddp_rank == 0
        else:
            self.master_process = True
            self.ddp_world_size = 1
        
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(1337)
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if self.master_process:
            os.makedirs(train_config.out_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = Transformer(model_config).to(self.device)
        
        # å°è¯•åŠ è½½é¢„è®­ç»ƒæƒé‡
        if os.path.exists(train_config.pretrained_model):
            pretrained_weights = load_pretrained_model(train_config.pretrained_model)
            if pretrained_weights is not None:
                try:
                    self.model.load_state_dict(pretrained_weights, strict=False)
                    print("æˆåŠŸåŠ è½½é¢„è®­ç»ƒæƒé‡")
                except Exception as e:
                    print(f"åŠ è½½é¢„è®­ç»ƒæƒé‡å¤±è´¥: {e}")
        
        # ç¼–è¯‘æ¨¡å‹
        if train_config.compile:
            print("ç¼–è¯‘æ¨¡å‹...")
            self.model = torch.compile(self.model)
        
        # DDPåŒ…è£…
        if self.ddp:
            self.model = DDP(self.model, device_ids=[self.ddp_local_rank])
        
        # è·å–åŸå§‹æ¨¡å‹å¼•ç”¨
        self.raw_model = self.model.module if self.ddp else self.model
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.optimizer = self._configure_optimizer()
        # åˆå§‹åŒ–SwanLabï¼ˆå¯é€‰ï¼‰
        if train_config.use_swanlab:
            try:
                swanlab.init(
                    project=train_config.swanlab_project,
                    experiment_name=f"tinyllm_{train_config.out_dir}",
                    config={
                        "model_config": model_config.__dict__,
                        "train_config": train_config.__dict__
                    }
                )
                self.use_swanlab = True
                print("SwanLabåˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"SwanLabåˆå§‹åŒ–å¤±è´¥: {e}")
                self.use_swanlab = False
        else:
            self.use_swanlab = False
        
        print(f"æ¨¡å‹å‚æ•°é‡: {self.raw_model.get_num_params():,}")

    
    def _configure_optimizer(self):
        """é…ç½®ä¼˜åŒ–å™¨"""
        # åˆ†ç¦»éœ€è¦æƒé‡è¡°å‡å’Œä¸éœ€è¦æƒé‡è¡°å‡çš„å‚æ•°
        decay = set()
        no_decay = set()
        whitelist_weight_modules = (torch.nn.Linear, )
        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)
        
        for mn, m in self.model.named_modules():
            for pn, p in m.named_parameters():
                fpn = '%s.%s' % (mn, pn) if mn else pn
                
                if pn.endswith('bias'):
                    no_decay.add(fpn)
                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):
                    decay.add(fpn)
                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):
                    no_decay.add(fpn)
                elif pn.endswith('weight') and 'norm' in mn.lower():  # ä»»ä½•åŒ…å«'norm'çš„æ¨¡å—çš„weightéƒ½ä¸è¡°å‡
                    no_decay.add(fpn)
        
        # è·å–æ‰€æœ‰å‚æ•°
        param_dict = {pn: p for pn, p in self.model.named_parameters()}
        
        # éªŒè¯æ‰€æœ‰å‚æ•°éƒ½è¢«åˆ†ç±»
        inter_params = decay & no_decay
        union_params = decay | no_decay
        assert len(inter_params) == 0, f"å‚æ•°ä¸èƒ½åŒæ—¶åœ¨decayå’Œno_decayä¸­: {inter_params}"
        assert len(param_dict.keys() - union_params) == 0, f"å‚æ•°æœªè¢«åˆ†ç±»: {param_dict.keys() - union_params}"
        
        # åˆ›å»ºä¼˜åŒ–å™¨ç»„ - ç›´æ¥ä½¿ç”¨åŸå§‹å‚æ•°å
        optim_groups = [
            {"params": [param_dict[pn] for pn in sorted(list(decay)) if pn in param_dict], "weight_decay": self.train_config.weight_decay},
            {"params": [param_dict[pn] for pn in sorted(list(no_decay)) if pn in param_dict], "weight_decay": 0.0},
        ]
        
        # ä½¿ç”¨AdamW
        optimizer = torch.optim.AdamW(
            optim_groups,
            lr=self.train_config.learning_rate,
            betas=(self.train_config.beta1, self.train_config.beta2)
        )
        
        return optimizer


    
    def get_lr(self, iter_num):
        """è·å–å­¦ä¹ ç‡ï¼ˆå¸¦é¢„çƒ­å’Œè¡°å‡ï¼‰"""
        # é¢„çƒ­é˜¶æ®µ
        if iter_num < self.train_config.warmup_iters:
            return self.train_config.learning_rate * iter_num / self.train_config.warmup_iters
        
        # è¡°å‡é˜¶æ®µ
        if iter_num > self.train_config.lr_decay_iters:
            return self.train_config.min_lr
        
        # ä½™å¼¦è¡°å‡
        decay_ratio = (iter_num - self.train_config.warmup_iters) / (self.train_config.lr_decay_iters - self.train_config.warmup_iters)
        assert 0 <= decay_ratio <= 1
        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))
        return self.train_config.min_lr + coeff * (self.train_config.learning_rate - self.train_config.min_lr)
    
    def train(self, get_batch_fn):
        """è®­ç»ƒä¸»å¾ªç¯"""
        iter_num = 0
        best_val_loss = float('inf')
        
        print("å¼€å§‹è®­ç»ƒ...")
        print(f"æœ€å¤§è¿­ä»£æ¬¡æ•°: {self.train_config.max_iters}")
        print(f"æ‰¹æ¬¡å¤§å°: {self.train_config.batch_size}")
        print(f"åºåˆ—é•¿åº¦: {self.train_config.max_seq_len}")
        
        while iter_num < self.train_config.max_iters:
            # è®¾ç½®å­¦ä¹ ç‡
            lr = self.get_lr(iter_num) if self.train_config.decay_lr else self.train_config.learning_rate
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = lr
            
            # è¯„ä¼°
            if iter_num % self.train_config.eval_interval == 0 and self.master_process:
                val_loss = self.evaluate(get_batch_fn)
                print(f"æ­¥éª¤ {iter_num}: éªŒè¯æŸå¤± {val_loss:.4f}, å­¦ä¹ ç‡ {lr:.6f}")
                
                # æ·»åŠ swanlabéªŒè¯æŸå¤±è®°å½•ï¼ˆåœ¨è¿™é‡ŒåŠ ï¼‰
                if hasattr(self, 'use_swanlab') and self.use_swanlab:
                    swanlab.log({
                        "val/loss": val_loss,
                        "val/best_loss": best_val_loss,
                        "train/step": iter_num
                    })
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    if iter_num > 0:
                        self.save_checkpoint(iter_num, best_val_loss)
                        print(f"ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: {best_val_loss:.4f}")

            
            # è®­ç»ƒæ­¥éª¤
            t0 = time.time()
            
            # å‰å‘ä¼ æ’­
            try:
                X, Y = get_batch_fn('train', self.train_config.batch_size)
                X, Y = X.to(self.device), Y.to(self.device)
                
                # ä½¿ç”¨æ··åˆç²¾åº¦
                with torch.amp.autocast(device_type=self.device.split(':')[0], dtype=self.dtype):
                    logits = self.model(X, Y)
                    loss = self.raw_model.last_loss
                
                # æ£€æŸ¥æŸå¤±æ˜¯å¦æœ‰æ•ˆ
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"è­¦å‘Š: æ£€æµ‹åˆ°æ— æ•ˆæŸå¤± {loss}, è·³è¿‡æ­¤æ­¥éª¤")
                    iter_num += 1
                    continue
                
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad(set_to_none=True)
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                if self.train_config.grad_clip != 0.0:
                    grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.train_config.grad_clip)
                
                # ä¼˜åŒ–å™¨æ­¥éª¤
                self.optimizer.step()
                
                # è®°å½•
                t1 = time.time()
                dt = t1 - t0
                
                if iter_num % self.train_config.log_interval == 0 and self.master_process:
                    lossf = loss.item()
                    if hasattr(self.raw_model, 'estimate_mfu'):
                        mfu = self.raw_model.estimate_mfu(self.train_config.batch_size * self.ddp_world_size, dt)
                        print(f"æ­¥éª¤ {iter_num}: æŸå¤± {lossf:.4f}, æ—¶é—´ {dt*1000:.2f}ms, MFU {mfu*100:.2f}%")
                        # æ·»åŠ swanlabæ—¥å¿—è®°å½•ï¼ˆåœ¨è¿™é‡ŒåŠ ï¼‰
                        if hasattr(self, 'use_swanlab') and self.use_swanlab:
                            swanlab.log({
                                "train/loss": lossf,
                                "train/learning_rate": lr,
                                "train/time_per_step": dt * 1000,
                                "train/mfu": mfu * 100,
                                "train/step": iter_num
                            })
                    else:
                        print(f"æ­¥éª¤ {iter_num}: æŸå¤± {lossf:.4f}, æ—¶é—´ {dt*1000:.2f}ms")
                        # æ·»åŠ swanlabæ—¥å¿—è®°å½•ï¼ˆåœ¨è¿™é‡ŒåŠ ï¼‰
                        if hasattr(self, 'use_swanlab') and self.use_swanlab:
                            swanlab.log({
                                "train/loss": lossf,
                                "train/learning_rate": lr,
                                "train/time_per_step": dt * 1000,
                                "train/step": iter_num
                            })
                
            except Exception as e:
                print(f"è®­ç»ƒæ­¥éª¤å‡ºé”™: {e}")
                iter_num += 1
                continue
            
            iter_num += 1
        
        # è®­ç»ƒç»“æŸï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹
        if self.master_process:
            self.save_checkpoint(iter_num, best_val_loss, final=True)
            print("è®­ç»ƒå®Œæˆï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹")
            if hasattr(self, 'use_swanlab') and self.use_swanlab:
                swanlab.finish()


        if self.ddp:
            destroy_process_group()
    
    @torch.no_grad()
    def evaluate(self, get_batch_fn):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        losses = torch.zeros(self.train_config.eval_iters)
        
        for k in range(self.train_config.eval_iters):
            try:
                X, Y = get_batch_fn('val', self.train_config.batch_size)
                X, Y = X.to(self.device), Y.to(self.device)
                
                with torch.amp.autocast(device_type=self.device.split(':')[0], dtype=self.dtype):
                    logits = self.model(X, Y)
                    loss = self.raw_model.last_loss
                
                losses[k] = loss.item()
                
            except Exception as e:
                print(f"è¯„ä¼°æ­¥éª¤å‡ºé”™: {e}")
                losses[k] = float('inf')
        
        self.model.train()
        return losses.mean()
    
    def save_checkpoint(self, iter_num, best_val_loss, final=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_name = 'final_checkpoint.ckpt' if final else 'checkpoint.ckpt'
        checkpoint_path = os.path.join(self.train_config.out_dir, checkpoint_name)
        
        # åˆ›å»ºæ£€æŸ¥ç‚¹å­—å…¸
        checkpoint = {
            'model_state_dict': self.raw_model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'model_config': self.model_config.__dict__,
            'train_config': self.train_config.__dict__,
            'iter_num': iter_num,
            'best_val_loss': best_val_loss,
        }
        
        torch.save(checkpoint, checkpoint_path)
        print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {checkpoint_path}")


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®
    model_config = ModelConfig(
        dim=288,
        n_layers=6,
        n_heads=6,
        n_kv_heads=6,  
        hidden_dim=1024,
        vocab_size=4096,
        max_seq_len=256
    )
    
    train_config = TrainerConfig()
    train_config.use_swanlab = True 
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    try:
        print("æ­£åœ¨åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        get_batch_fn, tokenizer = create_data_loader(
            train_config.data_dir,
            train_config.vocab_path,
            train_config.max_seq_len
        )
        print("æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        
        # æ›´æ–°è¯æ±‡è¡¨å¤§å°
        model_config.vocab_size = tokenizer.vocab_size
        print(f"æ›´æ–°è¯æ±‡è¡¨å¤§å°ä¸º: {model_config.vocab_size}")
        
        # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
        trainer = Trainer(model_config, train_config)
        trainer.train(get_batch_fn)
        
    except Exception as e:
        print(f"è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        
        # swanlabå…³é—­
        try:
            if 'trainer' in locals() and hasattr(trainer, 'use_swanlab') and trainer.use_swanlab:
                swanlab.finish()
        except:
            pass

if __name__ == '__main__':
    main()

```

![image.png](TinyLLM/image 1.png)

## â¯ï¸è®­ç»ƒç»“æœ

![SwanLab-Chart_2025-7-10_15_55_47.png](TinyLLM/SwanLab-Chart_2025-7-10_15_55_47.png)



