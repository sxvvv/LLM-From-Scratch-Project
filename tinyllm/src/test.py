# inference.py - ä¿®å¤ç‰ˆæœ¬
import torch
import torch.nn.functional as F
from model import Transformer
from data_utils import TinyStoriesTokenizer
from config.config import ModelConfig
import os
import argparse
import time

class TextGenerator:
    def __init__(self, checkpoint_path, vocab_path, device='cuda:4'):
        """
        åˆå§‹åŒ–æ–‡æœ¬ç”Ÿæˆå™¨
        Args:
            checkpoint_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
            vocab_path: è¯æ±‡è¡¨è·¯å¾„
            device: è®¾å¤‡
        """
        self.device = device
        
        # åŠ è½½tokenizer
        print("åŠ è½½tokenizer...")
        self.tokenizer = TinyStoriesTokenizer(vocab_path)
        print(f"âœ… tokenizeråŠ è½½æˆåŠŸï¼Œè¯æ±‡è¡¨å¤§å°: {self.tokenizer.vocab_size}")
        
        # åŠ è½½æ¨¡å‹
        print("åŠ è½½æ¨¡å‹...")
        self.model, self.model_config = self._load_model(checkpoint_path)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()
    
    def _load_model(self, checkpoint_path):
        """åŠ è½½æ¨¡å‹"""
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        
        print(f"ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹: {checkpoint_path}")
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        # æ‰“å°æ£€æŸ¥ç‚¹ä¿¡æ¯
        print(f"æ£€æŸ¥ç‚¹é”®: {list(checkpoint.keys())}")
        if 'iter_num' in checkpoint:
            print(f"è®­ç»ƒæ­¥æ•°: {checkpoint['iter_num']}")
        if 'best_val_loss' in checkpoint:
            print(f"æœ€ä½³éªŒè¯æŸå¤±: {checkpoint['best_val_loss']}")
        
        # è·å–æ¨¡å‹é…ç½®
        if 'model_config' in checkpoint:
            model_config_dict = checkpoint['model_config']
            model_config = ModelConfig(**model_config_dict)
            print("âœ… ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹é…ç½®")
        else:
            # å¦‚æœæ²¡æœ‰é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
            print("âš ï¸  æ£€æŸ¥ç‚¹ä¸­æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            model_config = ModelConfig(
                dim=288,
                n_layers=6,
                n_heads=6,
                n_kv_heads=6,
                hidden_dim=1024,
                vocab_size=self.tokenizer.vocab_size,
                max_seq_len=256
            )
        
        # ç¡®ä¿è¯æ±‡è¡¨å¤§å°åŒ¹é…
        model_config.vocab_size = self.tokenizer.vocab_size
        
        print(f"æ¨¡å‹é…ç½®: {model_config.__dict__}")
        
        # åˆ›å»ºæ¨¡å‹
        model = Transformer(model_config).to(self.device)
        
        # å¤„ç†æ¨¡å‹æƒé‡
        if 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
        else:
            state_dict = checkpoint
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†torch.compileçš„å‰ç¼€
        sample_key = list(state_dict.keys())[0]
        if sample_key.startswith('_orig_mod.'):
            print("ğŸ”§ æ£€æµ‹åˆ°torch.compileå‰ç¼€ï¼Œæ­£åœ¨å¤„ç†...")
            new_state_dict = {}
            for key, value in state_dict.items():
                new_key = key[10:]  # ç§»é™¤'_orig_mod.'å‰ç¼€
                new_state_dict[new_key] = value
            state_dict = new_state_dict
            print("âœ… å‰ç¼€å¤„ç†å®Œæˆ")
        
        # éªŒè¯æƒé‡é”®
        model_keys = set(model.state_dict().keys())
        checkpoint_keys = set(state_dict.keys())
        
        missing_keys = model_keys - checkpoint_keys
        unexpected_keys = checkpoint_keys - model_keys
        
        if missing_keys:
            print(f"âš ï¸  ç¼ºå°‘çš„æƒé‡é”®: {list(missing_keys)[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª
        if unexpected_keys:
            print(f"âš ï¸  æ„å¤–çš„æƒé‡é”®: {list(unexpected_keys)[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª
        
        # åŠ è½½æƒé‡
        try:
            model.load_state_dict(state_dict, strict=True)
            print("âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ ä¸¥æ ¼æ¨¡å¼åŠ è½½å¤±è´¥: {e}")
            try:
                model.load_state_dict(state_dict, strict=False)
                print("âš ï¸  éä¸¥æ ¼æ¨¡å¼åŠ è½½æˆåŠŸï¼Œå¯èƒ½å­˜åœ¨éƒ¨åˆ†æƒé‡ä¸åŒ¹é…")
            except Exception as e2:
                print(f"âŒ æ¨¡å‹æƒé‡åŠ è½½å®Œå…¨å¤±è´¥: {e2}")
                raise e2
        
        print(f"æ¨¡å‹å‚æ•°é‡: {model.get_num_params():,}")
        
        return model, model_config
    
    def generate(self, prompt, max_length=100, temperature=0.8, top_k=40, top_p=0.9, 
                 do_sample=True, repetition_penalty=1.1):
        """ç”Ÿæˆæ–‡æœ¬"""
        # ç¼–ç è¾“å…¥
        input_ids = self.tokenizer.encode(prompt, add_bos=True, add_eos=False)
        input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(self.device)
        
        # è®°å½•åŸå§‹é•¿åº¦
        original_length = input_ids.size(1)
        
        print(f"è¾“å…¥prompt: '{prompt}'")
        print(f"è¾“å…¥tokens: {input_ids.tolist()[0]}")
        print(f"è¾“å…¥é•¿åº¦: {original_length}")
        print("å¼€å§‹ç”Ÿæˆ...")
        
        generated_tokens = []
        
        with torch.no_grad():
            for step in range(max_length):
                # æ£€æŸ¥åºåˆ—é•¿åº¦ - ä¿®å¤è¿™é‡Œ
                if input_ids.size(1) >= self.model_config.max_seq_len:
                    print(f"è¾¾åˆ°æœ€å¤§åºåˆ—é•¿åº¦({self.model_config.max_seq_len})ï¼Œåœæ­¢ç”Ÿæˆ")
                    break
                
                try:
                    # è·å–logits
                    logits = self.model(input_ids)
                    
                    # åªå–æœ€åä¸€ä¸ªä½ç½®çš„logits
                    logits = logits[:, -1, :] / temperature
                    
                    # åº”ç”¨é‡å¤æƒ©ç½š
                    if repetition_penalty != 1.0:
                        logits = self._apply_repetition_penalty(logits, input_ids, repetition_penalty)
                    
                    # é‡‡æ ·ç­–ç•¥
                    if do_sample:
                        # Top-ké‡‡æ ·
                        if top_k > 0:
                            logits = self._top_k_filtering(logits, top_k)
                        
                        # Top-pé‡‡æ ·
                        if top_p < 1.0:
                            logits = self._top_p_filtering(logits, top_p)
                        
                        # éšæœºé‡‡æ ·
                        probs = F.softmax(logits, dim=-1)
                        next_token = torch.multinomial(probs, 1)
                    else:
                        # è´ªå¿ƒé‡‡æ ·
                        next_token = torch.argmax(logits, dim=-1, keepdim=True)
                    
                    next_token_id = next_token.item()
                    
                    # æ£€æŸ¥æ˜¯å¦ç”ŸæˆEOS
                    if next_token_id == self.tokenizer.eos_id:
                        print("ç”Ÿæˆäº†EOS tokenï¼Œåœæ­¢ç”Ÿæˆ")
                        break
                    
                    # è®°å½•ç”Ÿæˆçš„token
                    generated_tokens.append(next_token_id)
                    
                    # æ·»åŠ åˆ°åºåˆ—
                    input_ids = torch.cat([input_ids, next_token], dim=1)
                    
                    # æ‰“å°è¿›åº¦
                    if step % 10 == 0 or step < 5:
                        try:
                            current_text = self.tokenizer.decode(generated_tokens)
                            print(f"Step {step}: {current_text}")
                        except:
                            print(f"Step {step}: [è§£ç å¤±è´¥, token: {next_token_id}]")
                
                except Exception as e:
                    print(f"ç”Ÿæˆstep {step}æ—¶å‡ºé”™: {e}")
                    break
        
        # è§£ç ç”Ÿæˆçš„éƒ¨åˆ†
        try:
            generated_text = self.tokenizer.decode(generated_tokens)
            full_text = prompt + generated_text
        except Exception as e:
            print(f"è§£ç å¤±è´¥: {e}")
            generated_text = "[è§£ç å¤±è´¥]"
            full_text = prompt + generated_text
        
        print(f"\nç”Ÿæˆå®Œæˆï¼Œç”Ÿæˆäº† {len(generated_tokens)} ä¸ªtokens")
        print(f"ç”Ÿæˆçš„tokens: {generated_tokens}")
        
        return {
            'generated_text': generated_text,
            'full_text': full_text,
            'generated_tokens': generated_tokens,
            'input_tokens': input_ids[0, :original_length].tolist(),
            'total_tokens': len(generated_tokens)
        }
    
    def _apply_repetition_penalty(self, logits, input_ids, penalty):
        """åº”ç”¨é‡å¤æƒ©ç½š"""
        for token_id in set(input_ids[0].tolist()):
            logits[0, token_id] /= penalty
        return logits
    
    def _top_k_filtering(self, logits, top_k):
        """Top-kè¿‡æ»¤"""
        top_k = min(top_k, logits.size(-1))
        values, _ = torch.topk(logits, top_k)
        min_values = values[:, -1:].expand_as(logits)
        return torch.where(logits < min_values, 
                          torch.full_like(logits, float('-inf')), 
                          logits)
    
    def _top_p_filtering(self, logits, top_p):
        """Top-pè¿‡æ»¤"""
        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
        
        # ç§»é™¤è¶…è¿‡top_pçš„token
        sorted_indices_to_remove = cumulative_probs > top_p
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = 0
        
        # åˆ›å»ºmask
        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
        logits[indices_to_remove] = float('-inf')
        return logits

def main():
    parser = argparse.ArgumentParser(description='TinyLLMæ¨ç†æµ‹è¯•')
    parser.add_argument('--checkpoint', type=str, default='checkpoints/final_checkpoint.ckpt',
                        help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--vocab', type=str, default='/home/suxin/tinyllm/src/TinyStories4096.model',
                        help='è¯æ±‡è¡¨è·¯å¾„')
    parser.add_argument('--device', type=str, default='cuda:4',
                        help='è®¾å¤‡')
    parser.add_argument('--prompt', type=str, default='Once upon a time',
                        help='ç”Ÿæˆæç¤º')
    parser.add_argument('--max_length', type=int, default=50,
                        help='æœ€å¤§ç”Ÿæˆé•¿åº¦')
    parser.add_argument('--temperature', type=float, default=0.8,
                        help='æ¸©åº¦å‚æ•°')
    parser.add_argument('--top_k', type=int, default=40,
                        help='Top-ké‡‡æ ·')
    parser.add_argument('--top_p', type=float, default=0.9,
                        help='Top-pé‡‡æ ·')
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºç”Ÿæˆå™¨
        generator = TextGenerator(args.checkpoint, args.vocab, args.device)
        
        # å•æ¬¡ç”Ÿæˆæ¨¡å¼
        print("ğŸš€ å¼€å§‹ç”Ÿæˆæ–‡æœ¬")
        print("=" * 50)
        
        result = generator.generate(
            args.prompt,
            max_length=args.max_length,
            temperature=args.temperature,
            top_k=args.top_k,
            top_p=args.top_p
        )
        
        print(f"\nğŸ“ ç”Ÿæˆç»“æœ:")
        print(f"è¾“å…¥: {args.prompt}")
        print(f"ç”Ÿæˆ: {result['generated_text']}")
        print(f"å®Œæ•´: {result['full_text']}")
        print(f"ç”Ÿæˆtokensæ•°: {result['total_tokens']}")
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()
